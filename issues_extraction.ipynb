{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./AnacondaSetup/anaconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./AnacondaSetup/anaconda3/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in ./AnacondaSetup/anaconda3/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in ./AnacondaSetup/anaconda3/lib/python3.10/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./AnacondaSetup/anaconda3/lib/python3.10/site-packages (from nltk) (2022.7.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "unclosed token: line 95, column 4 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/AnacondaSetup/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3460\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[13], line 2\u001b[0m\n    nltk.download()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/AnacondaSetup/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:763\u001b[0m in \u001b[1;35mdownload\u001b[0m\n    self._interactive_download()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/AnacondaSetup/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:1113\u001b[0m in \u001b[1;35m_interactive_download\u001b[0m\n    DownloaderGUI(self).mainloop()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/AnacondaSetup/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:1410\u001b[0m in \u001b[1;35m__init__\u001b[0m\n    self._fill_table()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/AnacondaSetup/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:1746\u001b[0m in \u001b[1;35m_fill_table\u001b[0m\n    items = self._ds.collections()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/AnacondaSetup/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:594\u001b[0m in \u001b[1;35mcollections\u001b[0m\n    self._update_index()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/AnacondaSetup/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:952\u001b[0m in \u001b[1;35m_update_index\u001b[0m\n    ElementTree.parse(urlopen(self._url)).getroot()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/AnacondaSetup/anaconda3/lib/python3.10/xml/etree/ElementTree.py:1222\u001b[0m in \u001b[1;35mparse\u001b[0m\n    tree.parse(source, parser)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/AnacondaSetup/anaconda3/lib/python3.10/xml/etree/ElementTree.py:580\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    self._root = parser._parse_whole(source)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m unclosed token: line 95, column 4\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2731,
     "status": "ok",
     "timestamp": 1631703239003,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "lIYdn1woOS1n",
    "outputId": "d697000a-aca6-42b1-bbec-4a1c83c2f16b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1631703324050,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "4K8bPxtFC0vD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1403,
     "status": "ok",
     "timestamp": 1631703325435,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "QIeTUoqPC3o8"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"updated_dataset_81853_reviews1.csv\",encoding='utf-8')\n",
    "#data = pd.read_csv(\"testing-dataset-.csv\",encoding='utf-8')\n",
    "data['Base_Review'].replace('', np.nan, inplace=True)\n",
    "data.dropna(subset=['Base_Review'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1631703325443,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "spMFRz7dC7QU",
    "outputId": "63a74788-214c-4560-ffd1-c5f6de342db5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewer_Name</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Title_of_Review</th>\n",
       "      <th>Base_Review</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manuel Quilichini</td>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "      <td>This is a ripoff. And shame on Amazon for not ...</td>\n",
       "      <td>Don't buy this product. I purchased it for my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M Black</td>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "      <td>PDF Max Pro doesn't play well with marshmallow</td>\n",
       "      <td>Get an \"invalid file\" error since Marshmallow.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sandra</td>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "      <td>Worse app ever</td>\n",
       "      <td>I wouldn't even give it a one if I could. I wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ThunderDuck</td>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "      <td>I'd recommend trying to email the developer fi...</td>\n",
       "      <td>If I could give it less stars, I would! Does n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>Amazing For Markup</td>\n",
       "      <td>I have several apps that can open pdf files on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>2.0 out of 5 stars</td>\n",
       "      <td>Tools work great but loses files</td>\n",
       "      <td>The app works great for reading and annotating...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dale Hoch</td>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "      <td>Doesn't Work At All</td>\n",
       "      <td>Wow...  PDF Files are \"invalid\" and cannot be ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>J. Watson</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "      <td>Alright, but EZ PDF is better.  (One Reason Only)</td>\n",
       "      <td>This app is good at its job. There is still on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hoote</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "      <td>College Student review</td>\n",
       "      <td>Has a lot of good features. I'm not going to g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jan H.</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "      <td>Nice app except for one BIG thing</td>\n",
       "      <td>This would be a fantastic app except for one b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Reviewer_Name               Stars  \\\n",
       "0  Manuel Quilichini  1.0 out of 5 stars   \n",
       "1            M Black  1.0 out of 5 stars   \n",
       "2             Sandra  1.0 out of 5 stars   \n",
       "3        ThunderDuck  1.0 out of 5 stars   \n",
       "4           Jonathan  5.0 out of 5 stars   \n",
       "5    Amazon Customer  2.0 out of 5 stars   \n",
       "6          Dale Hoch  1.0 out of 5 stars   \n",
       "7          J. Watson  3.0 out of 5 stars   \n",
       "8              Hoote  3.0 out of 5 stars   \n",
       "9             Jan H.  3.0 out of 5 stars   \n",
       "\n",
       "                                     Title_of_Review  \\\n",
       "0  This is a ripoff. And shame on Amazon for not ...   \n",
       "1     PDF Max Pro doesn't play well with marshmallow   \n",
       "2                                     Worse app ever   \n",
       "3  I'd recommend trying to email the developer fi...   \n",
       "4                                 Amazing For Markup   \n",
       "5                   Tools work great but loses files   \n",
       "6                                Doesn't Work At All   \n",
       "7  Alright, but EZ PDF is better.  (One Reason Only)   \n",
       "8                             College Student review   \n",
       "9                  Nice app except for one BIG thing   \n",
       "\n",
       "                                         Base_Review Unnamed: 4 Unnamed: 5  \\\n",
       "0  Don't buy this product. I purchased it for my ...        NaN        NaN   \n",
       "1     Get an \"invalid file\" error since Marshmallow.        NaN        NaN   \n",
       "2  I wouldn't even give it a one if I could. I wi...        NaN        NaN   \n",
       "3  If I could give it less stars, I would! Does n...        NaN        NaN   \n",
       "4  I have several apps that can open pdf files on...        NaN        NaN   \n",
       "5  The app works great for reading and annotating...        NaN        NaN   \n",
       "6  Wow...  PDF Files are \"invalid\" and cannot be ...        NaN        NaN   \n",
       "7  This app is good at its job. There is still on...        NaN        NaN   \n",
       "8  Has a lot of good features. I'm not going to g...        NaN        NaN   \n",
       "9  This would be a fantastic app except for one b...        NaN        NaN   \n",
       "\n",
       "   Unnamed: 6  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "5         NaN  \n",
       "6         NaN  \n",
       "7         NaN  \n",
       "8         NaN  \n",
       "9         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1631703325445,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "4MtVPUOsC9Zk",
    "outputId": "ac4d69cd-1eff-43a4-fde4-b20d238aea71"
   },
   "outputs": [],
   "source": [
    "df = data.loc[:, ['Base_Review', 'Title_of_Review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1631703325452,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "CZMlucbQGvX6",
    "outputId": "fe4681ba-c3cd-432d-f80d-d80ea69e996f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base_Review</th>\n",
       "      <th>Title_of_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't buy this product. I purchased it for my ...</td>\n",
       "      <td>This is a ripoff. And shame on Amazon for not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Get an \"invalid file\" error since Marshmallow.</td>\n",
       "      <td>PDF Max Pro doesn't play well with marshmallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I wouldn't even give it a one if I could. I wi...</td>\n",
       "      <td>Worse app ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I could give it less stars, I would! Does n...</td>\n",
       "      <td>I'd recommend trying to email the developer fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have several apps that can open pdf files on...</td>\n",
       "      <td>Amazing For Markup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Base_Review  \\\n",
       "0  Don't buy this product. I purchased it for my ...   \n",
       "1     Get an \"invalid file\" error since Marshmallow.   \n",
       "2  I wouldn't even give it a one if I could. I wi...   \n",
       "3  If I could give it less stars, I would! Does n...   \n",
       "4  I have several apps that can open pdf files on...   \n",
       "\n",
       "                                     Title_of_Review  \n",
       "0  This is a ripoff. And shame on Amazon for not ...  \n",
       "1     PDF Max Pro doesn't play well with marshmallow  \n",
       "2                                     Worse app ever  \n",
       "3  I'd recommend trying to email the developer fi...  \n",
       "4                                 Amazing For Markup  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3598,
     "status": "ok",
     "timestamp": 1631703329008,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "EfRHPJopWI7I",
    "outputId": "ac6342de-2f80-4eba-b588-4c891a97749c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "#example for understand lamda which is equilavent to function\n",
    "(lambda x, y, z: x + y + z)(3, 8, 1)\n",
    "\n",
    "\n",
    "def add1(a,b,c):\n",
    "    return a+b+c\n",
    "\n",
    "sum= add1(4,8,1)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1631703329013,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "HB_7DqbojsTA"
   },
   "outputs": [],
   "source": [
    "data['cleaned'] = list(map(lambda x: x.lower().split(), data.Base_Review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(index):\n",
    "    example = data[data.index == index][['Base_Review']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        #print('Rationale_Type:', example[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got a new tablet with Marshmallow.  Apparently the app has been abandoned, because I get \"invalid file\" every time I try to read a file into the app.  This is not just a tablet problem, because I just checked my phone that has Marshmallow, and that also gives the same message.\n"
     ]
    }
   ],
   "source": [
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nekdilkhan/AnacondaSetup/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\<\\>\\|@____,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z =____#+_]')\n",
    "#STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def strip_html(text):\n",
    "    #print(text)\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_url(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    \n",
    "    text = strip_html(text) \n",
    "    text = remove_url(text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    #text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    #text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "data['Base_Review'] = data['Base_Review'].apply(clean_text)\n",
    "#df['Consumer complaint narrative'] = df['Consumer complaint narrative'].str.replace('\\d+', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i got a new tablet with marshmallow apparently the app has been abandoned  because i get invalid file every time i try to read a file into the app this is not just a tablet problem  because i just checked my phone that has marshmallow  and that also gives the same message\n"
     ]
    }
   ],
   "source": [
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Alice\n",
      "1 Bob\n",
      "2 Carl\n"
     ]
    }
   ],
   "source": [
    "lst = ['Alice', 'Bob', 'Carl']\n",
    "for i, j in enumerate(lst):\n",
    "    print(i, j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code for the debuging to identify errors\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "all_entities = []\n",
    "entity_chunks = {}\n",
    "previous_pos = None  \n",
    "\n",
    "def test1(fea):\n",
    "    #for fea in review:\n",
    "    print(fea)\n",
    "    #fea = fea.lower()\n",
    "    tokens = nltk.word_tokenize(fea)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    entity = nltk.ne_chunk(tags)\n",
    "    #print(entity)\n",
    "    for token, pos in entity:\n",
    "        #print(\"the token is:\",token)\n",
    "        #print(\"the pos is:\", pos)\n",
    "        if pos.startswith('VB'):\n",
    "            #print(\"inside if\")\n",
    "            all_entities.append(token)\n",
    "            #print(entity_chunks)\n",
    "            if entity_chunks.__contains__(token):\n",
    "                entity_chunks[token] += 1\n",
    "            else:\n",
    "                entity_chunks[token] = 1\n",
    "#print(entity_chunks)\n",
    "#print(all_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "unclosed token: line 92, column 4 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3460\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[18], line 8\u001b[0m\n    nltk.download('punkt')\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:777\u001b[0m in \u001b[1;35mdownload\u001b[0m\n    for msg in self.incr_download(info_or_id, download_dir, force):\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:629\u001b[0m in \u001b[1;35mincr_download\u001b[0m\n    info = self._info_or_id(info_or_id)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:603\u001b[0m in \u001b[1;35m_info_or_id\u001b[0m\n    return self.info(info_or_id)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:1009\u001b[0m in \u001b[1;35minfo\u001b[0m\n    self._update_index()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:952\u001b[0m in \u001b[1;35m_update_index\u001b[0m\n    ElementTree.parse(urlopen(self._url)).getroot()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/xml/etree/ElementTree.py:1222\u001b[0m in \u001b[1;35mparse\u001b[0m\n    tree.parse(source, parser)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.10/xml/etree/ElementTree.py:580\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    self._root = parser._parse_whole(source)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m unclosed token: line 92, column 4\n"
     ]
    }
   ],
   "source": [
    "#complete_text= []\n",
    "#for i in data['Base_Review']:\n",
    "    \n",
    "    #test1(i)\n",
    "    #complete_text.append(text)\n",
    "#print(complete_text)\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 83478,
     "status": "ok",
     "timestamp": 1631703412463,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "FnmzbYRuDBVq"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fea \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase_Review\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#print(fea)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     fea \u001b[38;5;241m=\u001b[39m fea\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m---> 14\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mword_tokenize(fea)\n\u001b[1;32m     15\u001b[0m     tags \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(tokens)\n\u001b[1;32m     16\u001b[0m     entity \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mne_chunk(tags)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extracting unique verbs from the dataset\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "all_verbs = []\n",
    "entity_chunks = {}\n",
    "previous_pos = None  \n",
    "\n",
    "for fea in data['Base_Review']:\n",
    "    #print(fea)\n",
    "    fea = fea.lower()\n",
    "    tokens = nltk.word_tokenize(fea)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    entity = nltk.ne_chunk(tags)\n",
    "    #print(entity)\n",
    "\n",
    "    for token, pos in entity:\n",
    "        #print(\"the token is:\",token)\n",
    "        #print(\"the pos is:\", pos)\n",
    "        if pos.startswith('VB'):\n",
    "            #print(\"inside if\")\n",
    "            all_verbs.append(token)\n",
    "            #print(entity_chunks)\n",
    "            if entity_chunks.__contains__(token):\n",
    "                entity_chunks[token] += 1\n",
    "            else:\n",
    "                entity_chunks[token] = 1\n",
    "#print(entity_chunks)\n",
    "#print(all_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1631703412478,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "o1nuEcHXDIEG",
    "outputId": "703826fc-4e96-4efb-cb97-97c2722a9e55"
   },
   "outputs": [],
   "source": [
    "#all_verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1631703412481,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "VGi2ye5JDIB6"
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1631703412483,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "2fcN8qI7DQJK"
   },
   "outputs": [],
   "source": [
    "csv_write_file = open('all_verbs.csv', 'w')\n",
    "writer = csv.writer(csv_write_file)\n",
    "for each in all_verbs:\n",
    "    #print(each)\n",
    "    line = []\n",
    "    line.append(each)\n",
    "#print (line)\n",
    "writer.writerow(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1631703412484,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "B6zrgm45DTcB"
   },
   "outputs": [],
   "source": [
    "verbs_result = sorted(entity_chunks.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1631703412487,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "_Uv2oH1zDY95",
    "outputId": "4fa94ae8-e599-4161-bcfb-3bdd29c7af50"
   },
   "outputs": [],
   "source": [
    "#verbs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1631703412488,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "kzp9w_UODcxR"
   },
   "outputs": [],
   "source": [
    "important_verbs_result = {v:k for k,v in verbs_result if v > 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1631703412489,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "VkrLrqcODiOK",
    "outputId": "dac04657-181f-402a-8c0a-fbb1b2fc579c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_verbs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "unclosed token: line 113, column 4 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3460\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[22], line 2\u001b[0m\n    nltk.download('punkt')\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:777\u001b[0m in \u001b[1;35mdownload\u001b[0m\n    for msg in self.incr_download(info_or_id, download_dir, force):\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:629\u001b[0m in \u001b[1;35mincr_download\u001b[0m\n    info = self._info_or_id(info_or_id)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:603\u001b[0m in \u001b[1;35m_info_or_id\u001b[0m\n    return self.info(info_or_id)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:1009\u001b[0m in \u001b[1;35minfo\u001b[0m\n    self._update_index()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/nltk/downloader.py:952\u001b[0m in \u001b[1;35m_update_index\u001b[0m\n    ElementTree.parse(urlopen(self._url)).getroot()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/xml/etree/ElementTree.py:1222\u001b[0m in \u001b[1;35mparse\u001b[0m\n    tree.parse(source, parser)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.10/xml/etree/ElementTree.py:580\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    self._root = parser._parse_whole(source)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m unclosed token: line 113, column 4\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/nekdilkhan/nltk_data'\n    - '/Users/nekdilkhan/anaconda3/nltk_data'\n    - '/Users/nekdilkhan/anaconda3/share/nltk_data'\n    - '/Users/nekdilkhan/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fea_N \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase_Review\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#print(fea)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     fea_N \u001b[38;5;241m=\u001b[39m fea_N\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m---> 14\u001b[0m     tokens_N \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfea_N\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     tags_N \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(tokens_N)\n\u001b[1;32m     16\u001b[0m     Nouns \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mne_chunk(tags_N)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/nekdilkhan/nltk_data'\n    - '/Users/nekdilkhan/anaconda3/nltk_data'\n    - '/Users/nekdilkhan/anaconda3/share/nltk_data'\n    - '/Users/nekdilkhan/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Extracting unique Nouns from the dataset\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "all_nouns = []\n",
    "entity_chunks_N = {}\n",
    "previous_pos_N = None  \n",
    "\n",
    "for fea_N in data['Base_Review']:\n",
    "    #print(fea)\n",
    "    fea_N = fea_N.lower()\n",
    "    tokens_N = nltk.word_tokenize(fea_N)\n",
    "    tags_N = nltk.pos_tag(tokens_N)\n",
    "    Nouns = nltk.ne_chunk(tags_N)\n",
    "    #print(entity)\n",
    "\n",
    "    for token_N, pos_N in Nouns:\n",
    "        #print(\"the token is:\",token)\n",
    "        #print(\"the pos is:\", pos)\n",
    "        if pos_N.startswith('NN'):\n",
    "            #print(\"inside if\")\n",
    "            all_nouns.append(token_N)\n",
    "            #print(entity_chunks)\n",
    "            if entity_chunks_N.__contains__(token_N):\n",
    "                entity_chunks_N[token_N] += 1\n",
    "            else:\n",
    "                entity_chunks_N[token_N] = 1\n",
    "#print(entity_chunks)\n",
    "#print(all_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_write_file_N = open('all_nouns.csv', 'w')\n",
    "writer_N = csv.writer(csv_write_file_N)\n",
    "for each_N in all_nouns:\n",
    "    #print(each)\n",
    "    line_N = []\n",
    "    line_N.append(each_N)\n",
    "#print (line)\n",
    "writer_N.writerow(line_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_result = sorted(entity_chunks_N.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_nouns_result = {v:k for k,v in nouns_result if v > 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_nouns_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19007,
     "status": "ok",
     "timestamp": 1631703431468,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "Vq8W_2vHDlUa"
   },
   "outputs": [],
   "source": [
    "# Extracting unique Adjectives from the dataset\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "all_adjectives = []\n",
    "entity_chunks_A = {}\n",
    "previous_pos_A = None  \n",
    "\n",
    "for fea_A in data['Base_Review']:\n",
    "    #print(fea)\n",
    "    fea_A = fea_A.lower()\n",
    "    tokens_A = nltk.word_tokenize(fea_A)\n",
    "    tags_A = nltk.pos_tag(tokens_A)\n",
    "    Adjectives = nltk.ne_chunk(tags_A)\n",
    "    #print(entity)\n",
    "\n",
    "    for token_A, pos_A in Adjectives:\n",
    "        #print(\"the token is:\",token)\n",
    "        #print(\"the pos is:\", pos)\n",
    "        if pos_A.startswith('JJ'):\n",
    "            #print(\"inside if\")\n",
    "            all_adjectives.append(token_A)\n",
    "            #print(entity_chunks)\n",
    "            if entity_chunks_A.__contains__(token_A):\n",
    "                entity_chunks_A[token_A] += 1\n",
    "            else:\n",
    "                entity_chunks_A[token_A] = 1\n",
    "#print(entity_chunks)\n",
    "#print(all_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1631703431477,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "8mCo40JzDsK5",
    "outputId": "73920e8d-57f1-48b9-c660-c9aff52d4931"
   },
   "outputs": [],
   "source": [
    "all_adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1631703431480,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "jnlGcr5uDx-I"
   },
   "outputs": [],
   "source": [
    "csv_write_file_A = open('all_adjectives.csv', 'w')\n",
    "writer_A = csv.writer(csv_write_file_A)\n",
    "for each_A in all_adjectives:\n",
    "    #print(each)\n",
    "    line_A = []\n",
    "    line_A.append(each_A)\n",
    "#print (line)\n",
    "writer_A.writerow(line_A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1631703431481,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "hKLLJn3ID00m"
   },
   "outputs": [],
   "source": [
    "adjectives_result = sorted(entity_chunks_A.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1631703431482,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "CH7XXnWFD2FQ"
   },
   "outputs": [],
   "source": [
    "adjectives_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1631703431483,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "6xrP7FqSD6mx",
    "outputId": "f6fef549-ee27-42ba-b552-0bddb431b062"
   },
   "outputs": [],
   "source": [
    "important_adjectives_result = {v:k for k,v in adjectives_result if v > 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1631703431484,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "nc0LQmyaD965",
    "outputId": "27746adc-0497-4b56-ddb2-385fea500393"
   },
   "outputs": [],
   "source": [
    "important_adjectives_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOKsWNOvEsHu"
   },
   "source": [
    "nur code hom run kigi lande ye paste ka **bold text** bold text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1642,
     "status": "ok",
     "timestamp": 1631703433108,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "NLCXMLqEECZg",
    "outputId": "28deaec6-c3b3-41c9-bdbc-ac3049b0b57a"
   },
   "outputs": [],
   "source": [
    "# create a DataFrame with the review data\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "relations = {}\n",
    "verbs = {}\n",
    "all_verb = []\n",
    "co_occur = {}\n",
    "lamda = 10\n",
    "\n",
    "for fea in data['Base_Review']:\n",
    "    fea = fea.lower()\n",
    "    tokens = nltk.word_tokenize(fea)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for index,(token, pos) in enumerate(tags):  \n",
    "        if token in entity_chunks:\n",
    "            i = index - 1;\n",
    "            while i >= 0 and index - i <= 5 and tags[i][1] != 'JJ':\n",
    "                i -= 1;\n",
    "            \n",
    "            if i >= 0 and tags[i][1] == 'JJ':\n",
    "                all_verbs.append(tags[i][0])\n",
    "                if tags[i][0] in verbs:\n",
    "                    verbs[tags[i][0]] += 1\n",
    "                else:\n",
    "                    verbs[tags[i][0]] = 1\n",
    "                \n",
    "                if tags[i][0] in co_occur:\n",
    "                    temp = co_occur[tags[i][0]]\n",
    "                    if tags[index][0] in temp:\n",
    "                        temp[tags[index][0]] += 1\n",
    "                    else:\n",
    "                        temp[tags[index][0]] = 1\n",
    "                else:\n",
    "                    co_occur[tags[i][0]] = {}\n",
    "                    \n",
    "                temp = \"\"\n",
    "                for j in range(i,index+1):\n",
    "                    temp += tags[j][0] + \" \"\n",
    "                    \n",
    "                if temp in relations:\n",
    "                    relations[temp] += 1\n",
    "                else:\n",
    "                    relations[temp] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1631703433110,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "KWTA_SlUdSjg"
   },
   "outputs": [],
   "source": [
    "csv_write_file = open('all_adjectives1.csv', 'w', newline='')\n",
    "writer = csv.writer(csv_write_file)\n",
    "for each in all_verbs:\n",
    "    line = []\n",
    "    line.append(each)\n",
    "    writer.writerow(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1631703433111,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "s67M2nPUdX2l"
   },
   "outputs": [],
   "source": [
    "relations_result = sorted(relations.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1631703433112,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "idp0b4lYdd2t"
   },
   "outputs": [],
   "source": [
    "verbs_result = sorted(verbs.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with the review data\n",
    "#df = pd.DataFrame(data)\n",
    "#print(\"the entity_chunks is\", entity_chunks)\n",
    "#print(\"the entity_chunks_N is\", entity_chunks_N)\n",
    "#print(\"the entity_chunks_A is\", entity_chunks_A)\n",
    "\n",
    "\n",
    "\n",
    "relations = {}\n",
    "verbs_relation = {}\n",
    "all_adjectives = []\n",
    "co_occur = {}\n",
    "lamda = 10\n",
    "print(\"the verbs_relation is\", verbs_relation)\n",
    "for fea in data['Base_Review']:\n",
    "    fea = fea.lower()\n",
    "    tokens = nltk.word_tokenize(fea)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    #print(\"the fea is\", fea)\n",
    "    #print(\"the tokens is\", tokens)\n",
    "    print(\"the tags is\", tags)\n",
    "    for index,(token, pos) in enumerate(tags):\n",
    "        #print(\"the index is\", index)\n",
    "        #print(\"the token is\", token)\n",
    "        #print(\"the pos is\", pos)\n",
    "        #print(\"the entity_chunks is\", entity_chunks)\n",
    "        if token in entity_chunks:\n",
    "            print(\"inside if\", token)\n",
    "            i = index - 1;\n",
    "            print(\"the tags[i][1] is\", tags[i][1])\n",
    "            while i >= 0 and index - i <= 5 and tags[i][1] != 'JJ':\n",
    "                i -= 1;\n",
    "            print(\"the value of i is\", i)\n",
    "            if i >= 0 and tags[i][1] == 'JJ':\n",
    "                all_adjectives.append(tags[i][0])\n",
    "                if tags[i][0] in verbs_relation:\n",
    "                    verbs_relation[tags[i][0]] += 1\n",
    "                else:\n",
    "                    verbs_relation[tags[i][0]] = 1\n",
    "                #print(\"co_occur\", co_occur)\n",
    "                if tags[i][0] in co_occur:\n",
    "                    temp = co_occur[tags[i][0]]\n",
    "                    if tags[index][0] in temp:\n",
    "                        temp[tags[index][0]] += 1\n",
    "                    else:\n",
    "                        temp[tags[index][0]] = 1\n",
    "                else:\n",
    "                    co_occur[tags[i][0]] = {}\n",
    "                    \n",
    "                temp = \"\"\n",
    "                for j in range(i,index+1):\n",
    "                    temp += tags[j][0] + \" \"\n",
    "                #print(\"relations\", relations)    \n",
    "                if temp in relations:\n",
    "                    relations[temp] += 1\n",
    "                else:\n",
    "                    relations[temp] = 1\n",
    "\n",
    "print(\"the verbs_relation is\", verbs_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = {}\n",
    "verbs_noun_adjective_relation = {}\n",
    "verbs_relation = {}\n",
    "noun_relation = {}\n",
    "all_adjectives = []\n",
    "co_occur = {}\n",
    "lamda = 10\n",
    "\n",
    "for fea in data['Base_Review']:\n",
    "    fea = fea.lower()\n",
    "    tokens = nltk.word_tokenize(fea)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for index, (token, pos) in enumerate(tags):\n",
    "        \n",
    "        if token in entity_chunks and entity_chunks_N:\n",
    "            i = index - 1;\n",
    "            while i >= 0 and index - i <= 5 and (tags[i][1] == 'NN' or tags[i][1].startswith('VB')):\n",
    "                i -= 1;\n",
    "            \n",
    "            if i >= 0 and tags[i][1] == 'JJ':\n",
    "                all_adjectives.append(tags[i][0])\n",
    "                \n",
    "                if tags[i][0] in verbs_noun_adjective_relation:\n",
    "                    verbs_noun_adjective_relation[tags[i][0]] += 1\n",
    "                else:\n",
    "                    verbs_noun_adjective_relation[tags[i][0]] = 1\n",
    "                \n",
    "                if tags[i][0] in co_occur:\n",
    "                    temp = co_occur[tags[i][0]]\n",
    "                    if tags[index][0] in temp:\n",
    "                        temp[tags[index][0]] += 1\n",
    "                    else:\n",
    "                        temp[tags[index][0]] = 1\n",
    "                else:\n",
    "                    co_occur[tags[i][0]] = {tags[index][0]: 1}\n",
    "                    \n",
    "                temp = \"\"\n",
    "                for j in range(i, index+1):\n",
    "                    temp += tags[j][0] + \" \"\n",
    "                \n",
    "                if temp in relations:\n",
    "                    relations[temp] += 1\n",
    "                else:\n",
    "                    relations[temp] = 1\n",
    "\n",
    "        if pos.startswith('VB'):\n",
    "            if token in verbs_relation:\n",
    "                verbs_relation[token] += 1\n",
    "            else:\n",
    "                verbs_relation[token] = 1\n",
    "\n",
    "        if pos.startswith('NN'):\n",
    "            if token in noun_relation:\n",
    "                noun_relation[token] += 1\n",
    "            else:\n",
    "                noun_relation[token] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_result = sorted(relations.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1631703433112,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "-a4wMpnkdhiC"
   },
   "outputs": [],
   "source": [
    "# Noun adjective relationship\n",
    "#df = pd.DataFrame(data)\n",
    "#print(\"the entity_chunks is\", entity_chunks)\n",
    "#print(\"the entity_chunks_N is\", entity_chunks_N)\n",
    "#print(\"the entity_chunks_A is\", entity_chunks_A)\n",
    "print(\"the verbs_relation is\", verbs)\n",
    "\n",
    "\n",
    "relations = {}\n",
    "verbs = {}\n",
    "all_verb = []\n",
    "co_occur = {}\n",
    "lamda = 10\n",
    "\n",
    "for fea in data['Base_Review']:\n",
    "    fea = fea.lower()\n",
    "    tokens = nltk.word_tokenize(fea)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    #print(\"the fea is\", fea)\n",
    "    #print(\"the tokens is\", tokens)\n",
    "    #print(\"the tags is\", tags)\n",
    "    for index,(token, pos) in enumerate(tags):\n",
    "        #print(\"the index is\", index)\n",
    "        #print(\"the token is\", token)\n",
    "        #print(\"the pos is\", pos)\n",
    "        #print(\"the entity_chunks is\", entity_chunks)\n",
    "        if token in entity_chunks_N:\n",
    "            print(\"inside if\", token)\n",
    "            i = index - 1;\n",
    "            print(\"the i is\", i)\n",
    "            while i >= 0 and index - i <= 5 and tags[i][1] != 'JJ':\n",
    "                i -= 1;\n",
    "            \n",
    "            if i >= 0 and tags[i][1] == 'JJ':\n",
    "                all_verbs.append(tags[i][0])\n",
    "                if tags[i][0] in verbs:\n",
    "                    verbs[tags[i][0]] += 1\n",
    "                else:\n",
    "                    verbs[tags[i][0]] = 1\n",
    "                #print(\"co_occur\", co_occur)\n",
    "                if tags[i][0] in co_occur:\n",
    "                    temp = co_occur[tags[i][0]]\n",
    "                    if tags[index][0] in temp:\n",
    "                        temp[tags[index][0]] += 1\n",
    "                    else:\n",
    "                        temp[tags[index][0]] = 1\n",
    "                else:\n",
    "                    co_occur[tags[i][0]] = {}\n",
    "                    \n",
    "                temp = \"\"\n",
    "                for j in range(i,index+1):\n",
    "                    temp += tags[j][0] + \" \"\n",
    "                #print(\"relations\", relations)    \n",
    "                if temp in relations:\n",
    "                    relations[temp] += 1\n",
    "                else:\n",
    "                    relations[temp] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1631703433114,
     "user": {
      "displayName": "Nek Dil Khan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK0EnXVVUeRBGYeGTiheS_eBC6f_p7GRA2DdiG7A=s64",
      "userId": "09105907598302335849"
     },
     "user_tz": -480
    },
    "id": "MDosE5iVdlnJ"
   },
   "outputs": [],
   "source": [
    "relations = {}\n",
    "verbs_noun_adjective_relation = {}\n",
    "co_occur = {}\n",
    "\n",
    "for fea in data['Base_Review']:\n",
    "    fea = fea.lower()\n",
    "    tokens = nltk.word_tokenize(fea)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    adjectives = []\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    \n",
    "    for index, (token, pos) in enumerate(tags):\n",
    "        if token in entity_chunks and entity_chunks_N:\n",
    "            if pos.startswith('JJ'):\n",
    "                adjectives.append(token)\n",
    "            elif pos.startswith('VB'):\n",
    "                verbs.append(token)\n",
    "            elif pos.startswith('NN'):\n",
    "                nouns.append(token)\n",
    "    \n",
    "    for adjective in adjectives:\n",
    "        for verb in verbs:\n",
    "            for noun in nouns:\n",
    "                # Store the co-occurrence of adjective, verb, and noun\n",
    "                if (adjective, verb, noun) in co_occur:\n",
    "                    co_occur[(adjective, verb, noun)] += 1\n",
    "                else:\n",
    "                    co_occur[(adjective, verb, noun)] = 1\n",
    "                \n",
    "                temp = f\"{adjective} {verb} {noun}\"\n",
    "                \n",
    "                if temp in relations:\n",
    "                    relations[temp] += 1\n",
    "                else:\n",
    "                    relations[temp] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_result = sorted(relations.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_relation_result = {v:k for k,v in relations_result if v > 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the relations dictionary\n",
    "print(\"Relations:\")\n",
    "for key, value in relations.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Display the co_occur dictionary\n",
    "#print(\"\\nCo-occurrence:\")\n",
    "#for key, value in co_occur.items():\n",
    "    #adjective, verb, noun = key\n",
    "    #print(f\"Adjective: {adjective}, Verb: {verb}, Noun: {noun}, Count: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "F_review (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
